# Comparative-Analysis-of-LLMs-for-Math-Education
Includes the Dataset and Codes used in the Research Project "Comparative Analysis of LLMs for Math Education"

Abstract:
  Artificial Intelligence (AI) has emerged as a powerful tool in education, with Large Language Models (LLMs) offering significant opportunities to support student learning and educator instruction. This study investigates the comparative performance of four LLMs in solving math problems across three key domains: Algebra, Calculus, and Statistics. A data set of 30 questions, ranging in complexity and domain, was used to evaluate the accuracy, clarity, and valuability of each model's responses. Evaluations were conducted by a human evaluators familiar with the subjects.

Results revealed that while all models performed similarly in terms of accuracy, the other two categories highlighted performance gaps. Meta AI emerged as the top performer, excelling in clarity and value, particularly in Calculus and Statistics. Julius AI demonstrated strength in solving complex word problems, whereas Gemini excelled at simpler problems. ChatGPT, however, consistently underperformed in all metrics, making it less reliable as a math resource. These findings offer insights into the capabilities of AI tools for math education, equipping educators and students with guidance on model selection based on the type and domain of the problem.

Found in the Repository:
 - The Question Dataset used in the Research
 - The Code used to get responses from ChatGPT
 - The Code used to get responses from Gemini

Find the Full Paper Here: 
 - [Comparative_Analysis_of_LLMs_for_Math_Education.pdf](https://github.com/user-attachments/files/19169677/Comparative_Analysis_of_LLMs_for_Math_Education.pdf)
